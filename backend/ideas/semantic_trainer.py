import logging
import os, random

import numpy as np
from sentence_transformers import SentenceTransformer, losses, InputExample
from torch.utils.data import DataLoader
from ideas.models import Idea, Connection

logger = logging.getLogger(__name__)


def fine_tune_model():
    ideas = list(Idea.objects.filter(used_for_training=False)[:50])
    if len(ideas) < 10:
        print("â³ Meno di 10 nuove idee, salto il training.")
        return

    # Carica modello base o ultimo salvato
    base_path = "models"
    latest_versions = sorted([f for f in os.listdir(base_path) if f.startswith("mindlink-v")])
    model_path = os.path.join(base_path, latest_versions[-1]) if latest_versions else "all-MiniLM-L6-v2"
    model = SentenceTransformer(model_path)

    # ğŸ”¹ Crea coppie (simili/dissimili)
    train_examples = []
    for i in ideas:
        positive = random.choice(ideas)
        negative = random.choice([x for x in ideas if x.id != positive.id])
        train_examples.append(InputExample(texts=[i.content, positive.content], label=1.0))
        train_examples.append(InputExample(texts=[i.content, negative.content], label=0.0))

    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)
    train_loss = losses.CosineSimilarityLoss(model)

    print("ğŸ¯ Inizio fine-tuning incrementale su 50 idee...")
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, show_progress_bar=True)

    # ğŸ”¹ Salva nuova versione del modello
    version = len(latest_versions) + 1
    new_path = os.path.join(base_path, f"mindlink-v{version}")
    model.save(new_path)
    print(f"âœ… Nuovo modello salvato in {new_path}")

    # ğŸ”¹ Marca le idee come usate
    Idea.objects.filter(id__in=[i.id for i in ideas]).update(used_for_training=True)
    print("ğŸ“˜ Marcate 50 idee come addestrate.")

    # ğŸ”¹ Aggiorna le connessioni pesate
    update_semantic_connections(model)
    print("âš–ï¸ Connessioni semantiche aggiornate con pesatura automatica.")


def update_semantic_connections(model=None, top_k: int = 5, strong_thr: float = 0.85, weak_thr: float = 0.6):
    """
    Aggiorna/crea connessioni semantiche pesate tra idee.
    ğŸ”¹ Usa gli embedding giÃ  salvati nel DB (evita ricodifica da testo)
    ğŸ”¹ Normalizza i vettori per cosine similarity stabile
    ğŸ”¹ Mantiene solo le top-k connessioni piÃ¹ forti per nodo
    ğŸ”¹ Logga metriche aggregate di qualitÃ 
    """

    ideas = list(Idea.objects.exclude(embedding=None))
    if len(ideas) < 2:
        logger.info("â„¹ï¸ Troppe poche idee per aggiornare connessioni.")
        return

    # === Embedding Matrix ===
    embeddings = np.array([np.array(i.embedding, dtype=float) for i in ideas])
    # Normalizzazione vettori (unit length)
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    embeddings = np.divide(embeddings, norms, out=np.zeros_like(embeddings), where=norms != 0)

    # === Similarity Matrix ===
    sim_matrix = np.dot(embeddings, embeddings.T)
    np.fill_diagonal(sim_matrix, 0.0)  # azzera self-similarity

    new_connections = 0
    updated_connections = 0
    total_strong = 0
    total_weak = 0

    # === Aggiornamento connessioni ===
    for i, source in enumerate(ideas):
        # Trova le top-k connessioni piÃ¹ simili
        top_indices = np.argsort(sim_matrix[i])[::-1][:top_k]

        for j in top_indices:
            sim = float(sim_matrix[i, j])
            if sim < weak_thr:
                continue

            target = ideas[j]
            ctype = "semantic_strong" if sim >= strong_thr else "semantic_weak"

            conn, created = Connection.objects.update_or_create(
                source=source,
                target=target,
                defaults={"strength": sim, "type": ctype}
            )

            if created:
                new_connections += 1
            else:
                updated_connections += 1

            if ctype == "semantic_strong":
                total_strong += 1
            else:
                total_weak += 1

    # === Metriche di qualitÃ  ===
    avg_sim = np.mean(sim_matrix[sim_matrix > 0])
    std_sim = np.std(sim_matrix[sim_matrix > 0])

    logger.info(
        f"ğŸ”— Connessioni aggiornate: {new_connections} nuove, {updated_connections} modificate. "
        f"(Strong: {total_strong}, Weak: {total_weak})"
    )
    logger.info(f"ğŸ“Š SimilaritÃ  media={avg_sim:.4f}, deviazione={std_sim:.4f}")

    print(f"âœ… Connessioni semantiche aggiornate: {new_connections} nuove, {updated_connections} aggiornate.")
    print(f"   Strong={total_strong}, Weak={total_weak}, AvgSim={avg_sim:.3f}")


from datetime import datetime

def force_training_now():
        """
        ğŸ”¹ Avvia il training istantaneo manualmente (richiamato dall'admin).
        """
        try:
            print(f"âš™ï¸ [Admin Trigger] Avvio training manuale alle {datetime.now().strftime('%H:%M:%S')} ...")
            fine_tune_model()
            return {"status": "ok", "message": "Training istantaneo completato con successo."}
        except Exception as e:
            return {"status": "error", "message": str(e)}